{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFT2UiIBZo7mD0qT0cIly2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m4tuuc/RAG_test/blob/main/RAG_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG o también Retrieval-augmented Generation es una técnica que combina la recuperación de información (retrieval) con la generación de texto.\n",
        "\n",
        "Este enfoque permite a los modelos de lenguaje aprovechar una base de datos externa de textos para complementar y enriquecer sus respuestas, mejorando la relevancia y exactitud de la información generada."
      ],
      "metadata": {
        "id": "l5akvIVSKz-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "GWWGBr83Gw1s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-core langchain-openai\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "ItnuC6-kKZBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./data\") #cargamos con nuestro set de datos"
      ],
      "metadata": {
        "id": "1xir341FGm-V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "df_data = df.astype(str)\n",
        "def preprocesar_texto(texto):\n",
        "    texto = re.sub(r\"[^a-zA-ZáéíóúÁÉÍÓÚñÑ\\s]\", \"\", texto)\n",
        "    # Normalizar acentos (ej: \"é\" → \"e\")\n",
        "    texto = unicodedata.normalize(\"NFKD\", texto).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
        "    return texto\n",
        "\n",
        "for columna in df_data.columns:\n",
        "  df_data[columna] = df_data[columna].apply(preprocesar_texto)"
      ],
      "metadata": {
        "id": "5-lyw4nqLq2X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Diviviendo la informacion en chunks\n",
        "Los documentos son divididos en pedacitos, o chunks, para que puedan ser correctamente procesados en este contexto."
      ],
      "metadata": {
        "id": "WGQQDCqsLP6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "documents = []\n",
        "for index, row in df_data.iterrows():\n",
        "  doc = Document(page_content=row['Sinopsis'], metadata={'source': f'row_{index}'})\n",
        "  documents.append(doc)"
      ],
      "metadata": {
        "id": "T22zpbOi3QpD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 450,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "splits = text_splitter.create_documents(df_data['Sinopsis'])\n",
        "\n",
        "for index, split in enumerate(splits):\n",
        "  print(f\"SPLIT + {index + 1}\")\n",
        "  print(\"--\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML31N9VKyp_U",
        "outputId": "764ef30a-b669-42d2-fc4d-6107395b7f7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPLIT + 1\n",
            "--\n",
            "SPLIT + 2\n",
            "--\n",
            "SPLIT + 3\n",
            "--\n",
            "SPLIT + 4\n",
            "--\n",
            "SPLIT + 5\n",
            "--\n",
            "SPLIT + 6\n",
            "--\n",
            "SPLIT + 7\n",
            "--\n",
            "SPLIT + 8\n",
            "--\n",
            "SPLIT + 9\n",
            "--\n",
            "SPLIT + 10\n",
            "--\n",
            "SPLIT + 11\n",
            "--\n",
            "SPLIT + 12\n",
            "--\n",
            "SPLIT + 13\n",
            "--\n",
            "SPLIT + 14\n",
            "--\n",
            "SPLIT + 15\n",
            "--\n",
            "SPLIT + 16\n",
            "--\n",
            "SPLIT + 17\n",
            "--\n",
            "SPLIT + 18\n",
            "--\n",
            "SPLIT + 19\n",
            "--\n",
            "SPLIT + 20\n",
            "--\n",
            "SPLIT + 21\n",
            "--\n",
            "SPLIT + 22\n",
            "--\n",
            "SPLIT + 23\n",
            "--\n",
            "SPLIT + 24\n",
            "--\n",
            "SPLIT + 25\n",
            "--\n",
            "SPLIT + 26\n",
            "--\n",
            "SPLIT + 27\n",
            "--\n",
            "SPLIT + 28\n",
            "--\n",
            "SPLIT + 29\n",
            "--\n",
            "SPLIT + 30\n",
            "--\n",
            "SPLIT + 31\n",
            "--\n",
            "SPLIT + 32\n",
            "--\n",
            "SPLIT + 33\n",
            "--\n",
            "SPLIT + 34\n",
            "--\n",
            "SPLIT + 35\n",
            "--\n",
            "SPLIT + 36\n",
            "--\n",
            "SPLIT + 37\n",
            "--\n",
            "SPLIT + 38\n",
            "--\n",
            "SPLIT + 39\n",
            "--\n",
            "SPLIT + 40\n",
            "--\n",
            "SPLIT + 41\n",
            "--\n",
            "SPLIT + 42\n",
            "--\n",
            "SPLIT + 43\n",
            "--\n",
            "SPLIT + 44\n",
            "--\n",
            "SPLIT + 45\n",
            "--\n",
            "SPLIT + 46\n",
            "--\n",
            "SPLIT + 47\n",
            "--\n",
            "SPLIT + 48\n",
            "--\n",
            "SPLIT + 49\n",
            "--\n",
            "SPLIT + 50\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardamos los chunks\n",
        "\n",
        "chunks = []\n",
        "for split in splits:\n",
        "  chunks.append(split.page_content)\n",
        "\n",
        "df = pd.DataFrame(chunks, columns=['Text'])"
      ],
      "metadata": {
        "id": "U3IcKiJ54bsF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generamos los embeddings\n",
        "Usamos Sentence Transformer para transformar la informacion a representacion numerica"
      ],
      "metadata": {
        "id": "tVPFQN-xLoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "#Bloque de codigo sacado de la documentacion.\n",
        "# 1. Load a pretrained Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# The sentences to encode\n",
        "sentences = df['Text']\n",
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings.shape)\n",
        "# [3, 384]\n",
        "\n",
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iOxoq6lIUV9",
        "outputId": "84f0f5c6-b705-4359-c2fd-6988d71b6280"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 384)\n",
            "tensor([[1.0000, 0.3984, 0.4225,  ..., 0.4011, 0.2624, 0.3261],\n",
            "        [0.3984, 1.0000, 0.1403,  ..., 0.3391, 0.3884, 0.3755],\n",
            "        [0.4225, 0.1403, 1.0000,  ..., 0.4320, 0.3002, 0.3271],\n",
            "        ...,\n",
            "        [0.4011, 0.3391, 0.4320,  ..., 1.0000, 0.4212, 0.5483],\n",
            "        [0.2624, 0.3884, 0.3002,  ..., 0.4212, 1.0000, 0.4506],\n",
            "        [0.3261, 0.3755, 0.3271,  ..., 0.5483, 0.4506, 1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Buscamos el chunk que nos interesa\n",
        "def find_best(query, dataframe, model, embeddings):\n",
        "  query_embedding = model.encode(query)\n",
        "  query_embedding = query_embedding.reshape(1, -1)\n",
        "  dot_product = np.dot(embeddings, query_embedding.T)\n",
        "  idx = np.argmax(dot_product)\n",
        "  return dataframe.iloc[idx]['Text']\n",
        "\n",
        "query = \"Hombre solo\"\n",
        "respuesta = find_best(query, df, model, embeddings)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UOixq5ZIxfv",
        "outputId": "446cd4ce-71dc-47c8-a5fa-9732cb276c3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Un hombre revive el mismo dia una y otra vez aprendiendo a aprovecharlo al maximo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generar la respuesta con un LLM\n",
        "Use la api de OpenAI, modelos como Llama, Claude tambien sirven."
      ],
      "metadata": {
        "id": "qWyvWEdoMjD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Para obtener la API desde Secrets en Google Colab. Si usas otro entorno debes adaptar esto.\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OpenAI')\n",
        "os.environ[\"OpenAI\"] = OPENAI_API_KEY\n",
        "\n",
        "\n",
        "\n",
        "llm_model = ChatOpenAI(model_name=\"gpt-4.1\", openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "3dg0m72PAUK7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "      Eres un bot servicial e informativo que arma pequeñas historias utilizando el texto referencia incluido a continuación.\n",
        "      Asegúrate de responder en una oración completa, siendo exhaustivo y proporcionando toda la información de fondo relevante.\n",
        "      Sin embargo, estás hablando con una audiencia no técnica, por lo que debes desglosar los conceptos complicados y mantener un tono amigable y conversacional.\n",
        "      Por favor, responde en el idioma de la pregunta.\n",
        "\n",
        "      PREGUNTA: '{query}'\n",
        "      TEXTO DE REFERENCIA: '{relevant_passage}'\n",
        "\n",
        "      RESPUESTA:\n",
        "    \"\"\"\n",
        "    ),\n",
        "])\n",
        "\n",
        "chain = prompt | llm_model | output_parser"
      ],
      "metadata": {
        "id": "GDItV0A8CU9k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\n",
        "    \"query\": query,\n",
        "    \"relevant_passage\": best_passage,\n",
        "    })"
      ],
      "metadata": {
        "id": "GPjoZd9dIibs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}